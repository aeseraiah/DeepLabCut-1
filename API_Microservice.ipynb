{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aeseraiah/DeepLabCut-1/blob/master/API_Microservice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERLSumnrA7Jc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f844d90-df45-44d4-8293-e277d042e06c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FnoieLOBLrt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c37118bc-de5c-4144-eeff-395fad4de4db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/gdrive/MyDrive/API_Microservice/testing_AI_Prediction-Microservice/'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive/API_Microservice/testing_AI_Prediction-Microservice/\n",
        "#os.environ['KAGGLE_CONFIG_DIR'] = data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlVsEt0mD1cV"
      },
      "outputs": [],
      "source": [
        "!pip install fuzzywuzzy\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.layers import Dense, Input, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from joblib import dump\n",
        "from joblib import load\n",
        "from tensorflow.keras.models import load_model\n",
        "import json\n",
        "from flask import request\n",
        "from src.load_input_data import *\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('inmate_history.csv') # data = dataframe = data structure in pandas library that is like a table or spreadsheet, where data is organzied into rows and columns\n",
        "columns = data.columns # columns = a pandas Index object = column labels of a dataframe\n",
        "cols = data.columns.tolist() # cols = a list of column labels (strings) from the 'data' dataframe\n",
        "\n",
        "features = data.drop('Suicide Committed', axis=1)\n",
        "features = features.columns.tolist()\n",
        "features = ','.join(features) #user input comes in the format of a string\n",
        "\n",
        "labels = data['Suicide Committed'] # labels = a series object\n",
        "labels = data[['Suicide Committed']] # labels = a new dataframe with a single column\n",
        "labels = labels.columns.tolist()\n",
        "labels = ','.join(labels)\n",
        "\n",
        "#strip() = removes leading and trailing whitespace characters from a string. \"   example feature  \" --> \"example feature\"\n",
        "#split(',') = splits a string (based on the the comma delimiter) into a list of substrings\n",
        "\n",
        "features = '  Meals Missed, Violent Offenses, Medication Refused, Depression, Days of Social '\n",
        "labels = 'Suicide Committed'\n",
        "feature_names = features.strip().split(',')\n",
        "label_names = labels.strip().split(',')\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "5CnQWpCsg0We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ba0065-61c2-4502-9c69-9f3f5e291bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Meals Missed (last 30 days)  Violent Offenses (last 30 days)  \\\n",
            "0                            10                               10   \n",
            "1                            28                               28   \n",
            "2                            25                               27   \n",
            "3                             0                                9   \n",
            "4                            17                               23   \n",
            "..                          ...                              ...   \n",
            "95                           13                                1   \n",
            "96                           30                               29   \n",
            "97                           26                               27   \n",
            "98                           21                               28   \n",
            "99                            1                                7   \n",
            "\n",
            "    Medication Refused (last 30 days) History of Depression  \\\n",
            "0                                  12                   yes   \n",
            "1                                  10                    no   \n",
            "2                                  16                   yes   \n",
            "3                                  14                   yes   \n",
            "4                                  23                    no   \n",
            "..                                ...                   ...   \n",
            "95                                  7                   yes   \n",
            "96                                 27                    no   \n",
            "97                                 27                   yes   \n",
            "98                                 29                    no   \n",
            "99                                  1                   yes   \n",
            "\n",
            "    Days of Social Isolation (last 30 days) Suicide Committed  \n",
            "0                                         1                no  \n",
            "1                                        28               yes  \n",
            "2                                        25               yes  \n",
            "3                                        12                no  \n",
            "4                                        29               yes  \n",
            "..                                      ...               ...  \n",
            "95                                       10                no  \n",
            "96                                       16               yes  \n",
            "97                                       15               yes  \n",
            "98                                       27               yes  \n",
            "99                                       14                no  \n",
            "\n",
            "[100 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# process is a module and extract is a function from the fuzzywuzzy, a library that finds the similiary betwene strings\n",
        "# selects the best match between a string (input_str) and a list of strings (choices)\n",
        "# scorer = the scoring algorithm to use for matching strogs\n",
        "# matches = a list a tuples, each tuple containing two elements: the matches string and its corresponding matching score\n",
        "\n",
        "input_str = 'example'\n",
        "choices = ['test1', 'example1', 'testmple']\n",
        "matches = process.extract(input_str, choices, scorer=fuzz.token_sort_ratio)\n",
        "# finds the best match from the 'matches' list using the 'max' function\n",
        "# It does this by comparing the second element of each tuple (the match score)\n",
        "\n",
        "# lambda x: x[1] --> for each element (tuple) 'x' in the matches list, the lambda function will be applied and te value at index 1 of x will be extracted\n",
        "# In the lambda function, x is the input paramter and x[1] is the expression evaluated and returned\n",
        "# best match will return the tuple with the best match score\n",
        "best_match = max(matches, key=lambda x: x[1])\n",
        "\n",
        "if best_match[1] >= 30:\n",
        "  print(best_match[0])\n",
        "else:\n",
        "  print('no good match found')\n"
      ],
      "metadata": {
        "id": "5jquhTRRghN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_label_match(input_str, choices):\n",
        "  matches = process.extract(input_str, choices, scorer=fuzz.token_sort_ratio)\n",
        "  best_match = max(matches, key=lambda x: x[1])\n",
        "\n",
        "  if best_match[1] >= 20:\n",
        "    return best_match[0]\n",
        "  else:\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "_wKWNyycwhkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_features = []\n",
        "for f in feature_names:\n",
        "  features_from_data = feature_label_match(f, cols)\n",
        "  input_features.append(features_from_data)\n",
        "\n",
        "# list comprehension:\n",
        "# input_features = [feature_label_match(f, cols) for f in feature_names]\n",
        "input_labels = [feature_label_match(l, cols) for l in label_names]\n",
        "# print(input_features)\n",
        "# print(input_labels)"
      ],
      "metadata": {
        "id": "IR8YqpiLuRWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_labels = data[input_labels[0]].unique()"
      ],
      "metadata": {
        "id": "3WuAcUt5H9WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label_encoder = LabelEncoder()\n",
        "# for idx in range(len(input_features)):\n",
        "#   # data[input_features[idx]].values returns a list of values in the given column\n",
        "#   if data[input_features[idx]].values.dtype == object:\n",
        "#     data[input_features[idx]] = label_encoder.fit_transform(data[input_features[idx]])\n",
        "# print(data[input_features[idx]])"
      ],
      "metadata": {
        "id": "LHvAMo1F6v2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvDAcXen4Ohb",
        "outputId": "370cf4ba-53be-4fd6-a99b-b037a2e49298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suicide Committed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling of missing data:\n",
        "\n",
        "# CREATE HUGE FUNCTION FOR DATA PREPROCESSING THAT TAKES IN TRAINING AND TESTING DATA SEPARATELY:\n",
        "\n",
        "# features = data.drop(input_labels, axis=1)\n",
        "# labels = data.drop(input_features, axis=1)\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "def convert_to_numerical(data, input_names):\n",
        "    label_encoder = LabelEncoder()\n",
        "    onehotencoder = OneHotEncoder()\n",
        "\n",
        "    encoded_features = []\n",
        "    object_features = []\n",
        "    object_labels = []\n",
        "    for name in range(len(input_names)):\n",
        "      # data[input_names[name]] is a pandas series\n",
        "        if data[input_names[name]].values.dtype == object:\n",
        "            if input_names == input_features:\n",
        "              object_features.append(input_names[name])\n",
        "            else:\n",
        "              object_labels.append(input_names[name])\n",
        "            # print(f\"the data type of the values of feature/label #{name} is categorical. We must change it to numerical before using it as input to our model\")\n",
        "            # label_encoder.fit_transform() takes these data types: 1-dimensional array-like input, such as a list, NumPy array, or Pandas Series\n",
        "            # label_encoder.fit_transform() returns an object that is the same data type as the input given. In this case, it returns a Pandas Series\n",
        "            data[input_names[name]] = label_encoder.fit_transform(data[input_names[name]])\n",
        "\n",
        "    # print(input_labels)\n",
        "    # print(input_features)\n",
        "    return object_features, object_labels\n",
        "\n",
        "def preprocess_two(data, input_features, input_labels, source):\n",
        "  data_types = data.dtypes\n",
        "  numerical_columns = []\n",
        "  categorical_columns = []\n",
        "  for column, dtype in data_types.items():\n",
        "    if dtype == 'int64' or dtype == 'float64':\n",
        "      numerical_columns.append(column)\n",
        "    else:\n",
        "      categorical_columns.append(column)\n",
        "\n",
        "  # Handling of missing numerical data:\n",
        "  if numerical_columns:\n",
        "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "    imputer = imputer.fit(data[numerical_columns])\n",
        "    data[numerical_columns] = imputer.transform(data[numerical_columns])\n",
        "  # Handling of missing categorical data:\n",
        "  elif categorical_columns:\n",
        "    imputer = SimpleImputer(strategy='most_frequent')\n",
        "    imputer = imputer.fit(data[categorical_columns])\n",
        "    # most_frequent_values = imputer.statistics_\n",
        "    # print(most_frequent_values)\n",
        "    data[categorical_columns] = imputer.transform(data[categorical_columns])\n",
        "\n",
        "\n",
        "  if source == 'X_train' or source == 'X_test':\n",
        "    object_features, object_labels = convert_to_numerical(data, input_features)\n",
        "    returned_data = pd.get_dummies(data)\n",
        "    # returned_data = pd.get_dummies(data.drop(input_labels, axis=1), columns=object_features)\n",
        "  else:\n",
        "    object_features, object_labels = convert_to_numerical(data, input_labels)\n",
        "    # returned_data = pd.get_dummies(data.drop(input_features, axis=1), columns=object_labels)\n",
        "    returned_data = pd.get_dummies(data)\n",
        "    returned_labels = pd.get_dummies(data)\n",
        "\n",
        "\n",
        "  # SCALE DATA:\n",
        "  min_max_scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "  scaled_data = min_max_scaler.fit_transform(returned_data)\n",
        "\n",
        "  # Normalizing data such that featues have a zero mean and std dev of 1:\n",
        "  scaler = StandardScaler()\n",
        "  scaled_data = scaler.fit_transform(scaled_data)\n",
        "\n",
        "  # scaler.fit_transform() takes as a 2-d array-like oject as input, such as numpy arrays, datframes\n",
        "  # 'scaler.fit_transform()' returns a numpy array\n",
        "  return scaled_data\n",
        "\n",
        "def preprocess(file_path, features, labels):\n",
        "    # Mapping of file extensions to read functions\n",
        "    read_functions = {\n",
        "        'csv': pd.read_csv,\n",
        "        'xlsx': pd.read_excel,\n",
        "        'xls': pd.read_excel,\n",
        "        'json': pd.read_json,\n",
        "        'txt': pd.read_csv,  # Example: plain text file\n",
        "        'h5': pd.read_hdf,   # Example: HDF5 file\n",
        "        'parquet': pd.read_parquet  # Example: Parquet file\n",
        "        # Add more file formats and read functions as needed\n",
        "    }\n",
        "\n",
        "    file_extension = file_path.split('.')[-1].lower()\n",
        "\n",
        "    if file_extension in read_functions:\n",
        "        read_function = read_functions[file_extension]\n",
        "        data = read_function(file_path)\n",
        "    else:\n",
        "        raise ValueError(f\"File extension not given or an unsupported file format: '{file_extension}' has been provided\")\n",
        "\n",
        "    col_names = data.columns.tolist()\n",
        "    print(col_names)\n",
        "    feature_names = features.strip().split(',')\n",
        "    label_names = labels.strip().split(',')\n",
        "\n",
        "    # Perform fuzzy matching to find the best matches for features and labels\n",
        "    # 'input_features' is a list of avaiable features the model will use\n",
        "    input_features = [feature_label_match(f,col_names) for f in feature_names]\n",
        "    input_labels = [feature_label_match(l, col_names) for l in label_names]\n",
        "\n",
        "\n",
        "    features = data.drop(input_labels, axis=1)\n",
        "    labels = data.drop(input_features, axis=1)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    X_train_preprocessed = preprocess_two(X_train, input_features, input_labels, source='X_train')\n",
        "    X_test_preprocessed = preprocess_two(X_test, input_features, input_labels, source='X_test')\n",
        "    y_train_preprocessed = preprocess_two(y_train, input_features, input_labels, source='y_train')\n",
        "    y_test_preprocessed = preprocess_two(y_test, input_features, input_labels, source='y_test')\n",
        "\n",
        "    return X_train_preprocessed, X_test_preprocessed, y_train_preprocessed, y_test_preprocessed\n"
      ],
      "metadata": {
        "id": "KlKmOqsHr8A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(file_path, features, labels):\n",
        "    # Mapping of file extensions to read functions\n",
        "    read_functions = {\n",
        "        'csv': pd.read_csv,\n",
        "        'xlsx': pd.read_excel,\n",
        "        'xls': pd.read_excel,\n",
        "        'json': pd.read_json,\n",
        "        'txt': pd.read_csv,  # Example: plain text file\n",
        "        'h5': pd.read_hdf,   # Example: HDF5 file\n",
        "        'parquet': pd.read_parquet  # Example: Parquet file\n",
        "        # Add more file formats and read functions as needed\n",
        "    }\n",
        "\n",
        "    file_extension = file_path.split('.')[-1].lower()\n",
        "\n",
        "    if file_extension in read_functions:\n",
        "        # read_function = pd.read_csv\n",
        "        read_function = read_functions[file_extension]\n",
        "        # data = pd.read_csv(file_path)\n",
        "        data = read_function(file_path)\n",
        "    else:\n",
        "        raise ValueError(f\"File extension not given or an unsupported file format: '{file_extension}' has been provided\")\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "qKHA_gZ14pDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "read_functions = {\n",
        "    'csv': pd.read_csv,\n",
        "    'xlsx': pd.read_excel,\n",
        "    'xls': pd.read_excel,\n",
        "    'json': pd.read_json,\n",
        "    'txt': pd.read_csv,  # Example: plain text file\n",
        "    'h5': pd.read_hdf,   # Example: HDF5 file\n",
        "    'parquet': pd.read_parquet  # Example: Parquet file\n",
        "    # Add more file formats and read functions as needed\n",
        "}\n",
        "\n",
        "df = pd.read_excel\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM4y44Kq62mE",
        "outputId": "fa5e589d-699b-481e-8b9b-0e0dcddd0979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function read_excel at 0x7f5ac3579fc0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = preprocess('inmate_history.csv', features, labels)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjSkLxY54qvM",
        "outputId": "0fb3be7d-98df-4273-cadf-342751e532a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function read_csv at 0x7f5ac3578e50>\n",
            "    Meals Missed (last 30 days)  Violent Offenses (last 30 days)  \\\n",
            "0                            10                               10   \n",
            "1                            28                               28   \n",
            "2                            25                               27   \n",
            "3                             0                                9   \n",
            "4                            17                               23   \n",
            "..                          ...                              ...   \n",
            "95                           13                                1   \n",
            "96                           30                               29   \n",
            "97                           26                               27   \n",
            "98                           21                               28   \n",
            "99                            1                                7   \n",
            "\n",
            "    Medication Refused (last 30 days) History of Depression  \\\n",
            "0                                  12                   yes   \n",
            "1                                  10                    no   \n",
            "2                                  16                   yes   \n",
            "3                                  14                   yes   \n",
            "4                                  23                    no   \n",
            "..                                ...                   ...   \n",
            "95                                  7                   yes   \n",
            "96                                 27                    no   \n",
            "97                                 27                   yes   \n",
            "98                                 29                    no   \n",
            "99                                  1                   yes   \n",
            "\n",
            "    Days of Social Isolation (last 30 days) Suicide Committed  \n",
            "0                                         1                no  \n",
            "1                                        28               yes  \n",
            "2                                        25               yes  \n",
            "3                                        12                no  \n",
            "4                                        29               yes  \n",
            "..                                      ...               ...  \n",
            "95                                       10                no  \n",
            "96                                       16               yes  \n",
            "97                                       15               yes  \n",
            "98                                       27               yes  \n",
            "99                                       14                no  \n",
            "\n",
            "[100 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_preprocessed, X_test_preprocessed, y_train_preprocessed, y_test_preprocessed = preprocess('inmate_history.csv', features, labels)\n",
        "print(X_train_preprocessed)"
      ],
      "metadata": {
        "id": "46sx2g1yjtIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "onehotencoder = OneHotEncoder()\n",
        "\n",
        "encoded_features = []\n",
        "object_features = []\n",
        "object_labels = []\n",
        "\n",
        "def convert_to_numerical(data, input_names):\n",
        "    for name in range(len(input_names)):\n",
        "      # data[input_names[name]] is a pandas series\n",
        "        if data[input_names[name]].values.dtype == object:\n",
        "            if input_names == input_features:\n",
        "              object_features.append(input_names[name])\n",
        "            else:\n",
        "              object_labels.append(input_names[name])\n",
        "            # print(f\"the data type of the values of feature/label #{name} is categorical. We must change it to numerical before using it as input to our model\")\n",
        "            # label_encoder.fit_transform() takes these data types: 1-dimensional array-like input, such as a list, NumPy array, or Pandas Series\n",
        "            # label_encoder.fit_transform() returns an object that is the same data type as the input given. In this case, it returns a Pandas Series\n",
        "            data[input_names[name]] = label_encoder.fit_transform(data[input_names[name]])\n",
        "\n",
        "\n",
        "\n",
        "convert_to_numerical(data, input_features)\n",
        "convert_to_numerical(data, input_labels)\n",
        "\n",
        "# one-hot encoding:\n",
        "# each label is transformed into a row of zeros except for the index corresponding to the label, which is set to 1\n",
        "returned_features = pd.get_dummies(data.drop(input_labels, axis=1), columns=object_features)\n",
        "returned_labels = pd.get_dummies(data.drop(input_features, axis=1), columns=object_labels)\n",
        "\n",
        "\n",
        "\n",
        "# LABEL_ENCODER VS TO_CATEGORICAL:\n",
        "# Use 'label_encoder' for features (numerical encoding)\n",
        "# Use 'label_encoder' followed by 'to_categorical' for labels. Just using 'label_encoder' ensures that each class is represented by a unique integer value.\n",
        "# However, numerical encoding introduces ordinal relationship between the classes, which might not be appropriate for certain classification tasks.\n",
        "# 'to_categorical' performs one-hot encoding, which represents each class as a binary vector\n",
        "# In this specific instance, there is not importance of order for the labels 'yes' and 'no'. This is because 'yes' and 'no' are not directly related to one another\n",
        "# If the classes were instead 'low', 'medium', 'high', then only performing numerical encoding might allow the model to learn the ordinal relationship between these labels/classes\n"
      ],
      "metadata": {
        "id": "R9p8X40e3m2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# returned_labels = data[input_labels]\n",
        "# 'to_categorical' takes an array-like object as input, such as a numpy array or a pandas series\n",
        "# 'to_categorical' returns a numpy array\n",
        "# returned_labels = to_categorical(returned_labels)\n",
        "\n",
        "# Difference between scaling and Normalization:\n",
        "# Scaling (transforming data into a (0,1) or (-1,1) range). Ypu would typically use MinMaxScaler() to do this\n",
        "# Normalization (transofmrs data into having a zero mean and std dev. of 1. The range of the transformed data does not have to be between 0 and 1)\n",
        "\n",
        "def scale_normalize_data(data):\n",
        "  # Scaling data into range (-1,1):\n",
        "  min_max_scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "  scaled_features = min_max_scaler.fit_transform(data)\n",
        "\n",
        "  # Normalizing data such that featues have a zero mean and std dev of 1:\n",
        "  scaler = StandardScaler()\n",
        "  scaled_features = scaler.fit_transform(scaled_features)\n",
        "\n",
        "  # scaler.fit_transform() takes as a 2-d array-like oject as input, such as numpy arrays, datframes\n",
        "  # 'scaler.fit_transform()' returns a numpy array\n",
        "  return scaled_features\n",
        "\n",
        "scaled_features = scale_normalize_data(returned_features)\n",
        "print(scaled_features)\n"
      ],
      "metadata": {
        "id": "uXCyUpndUBda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)"
      ],
      "metadata": {
        "id": "N6OuihJX_Gu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(scaled_features, returned_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=X_train_preprocessed.shape[1]))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(returned_labels.shape[1], activation='softmax'))\n",
        "\n",
        "checkpoint = ModelCheckpoint('weights_best.hdf5', save_best_only=True)\n",
        "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_preprocessed, y_train_preprocessed, epochs=10, batch_size=32, validation_data=(X_test_preprocessed, y_test_preprocessed), callbacks=[checkpoint, early_stopping])\n",
        "\n",
        "model.load_weights('weights_best.hdf5')\n",
        "\n",
        "model.save(\"user_model.h5\")\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "5iGptGn6UBTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape) # returns (80% of the num of subjects (rows), the num of features)\n",
        "print(y_train.shape) # returns (80% of the num of subjects (rows), the num of classes)\n",
        "print(X_test.shape) # returns (20% of the num of subjects (rows), the num of features)\n",
        "print(y_test.shape) # returns (20% of the num of subjects (rows), the num of classes)\n",
        "print(returned_labels.shape)"
      ],
      "metadata": {
        "id": "GiLV7T5EJcBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DizBVvldH6T"
      },
      "outputs": [],
      "source": [
        "histo1 = data.hist(data.columns)\n",
        "print(histo1)\n",
        "\n",
        "#x axis of histo1 represents the length (in pixels) of the Major Axis of the bean\n",
        "#y axis of histo1 represents the count of that particular length. For example, histo1 shows that there is a peak of 4800 for a length of 250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W26KG3xUVyB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}